{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "house_price_modified_10012020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DineshReddy0530/Grepthor/blob/master/house_price_modified_10012020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqpq_wqokEr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMl3aN4OkV_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('housepricedata.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fTb6IT0l8q7",
        "colab_type": "code",
        "outputId": "58398580-7a14-433c-e84e-e9331ee989d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "dataset = df.values\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8450,     7,     5, ...,     0,   548,     1],\n",
              "       [ 9600,     6,     8, ...,     1,   460,     1],\n",
              "       [11250,     7,     5, ...,     1,   608,     1],\n",
              "       ...,\n",
              "       [ 9042,     7,     9, ...,     2,   252,     1],\n",
              "       [ 9717,     5,     6, ...,     0,   240,     0],\n",
              "       [ 9937,     5,     6, ...,     0,   276,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHdvNRO2nuUU",
        "colab_type": "code",
        "outputId": "c1974cd5-e6e7-4c88-92b0-90228f90d349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhQ5MeK2n8Qs",
        "colab_type": "code",
        "outputId": "f42fb6fa-2b24-4b88-cc23-80fab9b40075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>AboveMedianPrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8450</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>856</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>548</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9600</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1262</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11250</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>920</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>608</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9550</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>756</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>642</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14260</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1145</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>836</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   LotArea  OverallQual  OverallCond  ...  Fireplaces  GarageArea  AboveMedianPrice\n",
              "0     8450            7            5  ...           0         548                 1\n",
              "1     9600            6            8  ...           1         460                 1\n",
              "2    11250            7            5  ...           1         608                 1\n",
              "3     9550            7            5  ...           1         642                 0\n",
              "4    14260            8            5  ...           1         836                 1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JArIX12mico",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = dataset[:,0:10]\n",
        "Y = dataset[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVMPK_Ccmz1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH7tRM5qZWt8",
        "colab_type": "code",
        "outputId": "f878074c-3dd6-4278-e5f2-ee12488de536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X_scale"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0334198 , 0.66666667, 0.5       , ..., 0.5       , 0.        ,\n",
              "        0.3864598 ],\n",
              "       [0.03879502, 0.55555556, 0.875     , ..., 0.33333333, 0.33333333,\n",
              "        0.32440056],\n",
              "       [0.04650728, 0.66666667, 0.5       , ..., 0.33333333, 0.33333333,\n",
              "        0.42877292],\n",
              "       ...,\n",
              "       [0.03618687, 0.66666667, 1.        , ..., 0.58333333, 0.66666667,\n",
              "        0.17771509],\n",
              "       [0.03934189, 0.44444444, 0.625     , ..., 0.25      , 0.        ,\n",
              "        0.16925247],\n",
              "       [0.04037019, 0.44444444, 0.625     , ..., 0.33333333, 0.        ,\n",
              "        0.19464034]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVm714dzZWO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUnQ9BUNaimn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cf8REOZawrO",
        "colab_type": "text"
      },
      "source": [
        "## In summary:\n",
        "we now have a total of six variables for our datasets we will use:\n",
        "\n",
        "<b>X_train </b>(10 input features, 70% of full dataset) <br>\n",
        "<b>X_val</b> (10 input features, 15% of full dataset) <br>\n",
        "<b>X_test</b> (10 input features, 15% of full dataset) <br>\n",
        "<b>Y_train</b> (1 label, 70% of full dataset) <br>\n",
        "<b>Y_val</b> (1 label, 15% of full dataset) <br>\n",
        "<b>Y_test</b> (1 label, 15% of full dataset) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4RYFDUqdiT9",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Layers\n",
        "Hidden layer 1: <b>32 neurons</b>, ReLU activation <br>\n",
        "Hidden layer 2: <b>32 neurons</b>, ReLU activation<br>\n",
        "Output Layer: <b>1 neuron</b>, Sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UljXkjMValYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEfGr_iXfIFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(10,)),\n",
        "    \n",
        "    Dense(64, activation='relu'),\n",
        "    \n",
        "    Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHK02segt0x",
        "colab_type": "text"
      },
      "source": [
        "## Configure the model :\n",
        "\n",
        "<b>Algorithm</b> you want to use to do the <i>optimization</i><br>\n",
        "<b>Loss function</b> to use<br>\n",
        "<b>Metrics </b>you want to track apart from the loss function<br><br>\n",
        "Configuring the model with these settings requires us to call the function <b>model.compile</b> :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrL_rvlefTck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfTn0SZGfsgf",
        "colab_type": "code",
        "outputId": "8b86e33c-2f86-48f3-c5fe-b17222c9018c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=120,\n",
        "          validation_data=(X_val, Y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1022 samples, validate on 219 samples\n",
            "Epoch 1/120\n",
            "1022/1022 [==============================] - 1s 493us/step - loss: 0.7025 - acc: 0.4403 - val_loss: 0.6985 - val_acc: 0.5114\n",
            "Epoch 2/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.6954 - acc: 0.4971 - val_loss: 0.6912 - val_acc: 0.5662\n",
            "Epoch 3/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.6888 - acc: 0.5294 - val_loss: 0.6840 - val_acc: 0.5708\n",
            "Epoch 4/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.6821 - acc: 0.5313 - val_loss: 0.6771 - val_acc: 0.5982\n",
            "Epoch 5/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.6759 - acc: 0.6155 - val_loss: 0.6704 - val_acc: 0.6301\n",
            "Epoch 6/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.6700 - acc: 0.6605 - val_loss: 0.6642 - val_acc: 0.6895\n",
            "Epoch 7/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.6647 - acc: 0.6986 - val_loss: 0.6587 - val_acc: 0.7352\n",
            "Epoch 8/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.6597 - acc: 0.7192 - val_loss: 0.6534 - val_acc: 0.7580\n",
            "Epoch 9/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.6546 - acc: 0.7299 - val_loss: 0.6483 - val_acc: 0.7991\n",
            "Epoch 10/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.6493 - acc: 0.7642 - val_loss: 0.6425 - val_acc: 0.8311\n",
            "Epoch 11/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.6436 - acc: 0.8043 - val_loss: 0.6359 - val_acc: 0.8447\n",
            "Epoch 12/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.6376 - acc: 0.8023 - val_loss: 0.6296 - val_acc: 0.8539\n",
            "Epoch 13/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.6316 - acc: 0.8160 - val_loss: 0.6232 - val_acc: 0.8493\n",
            "Epoch 14/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.6256 - acc: 0.8151 - val_loss: 0.6172 - val_acc: 0.8767\n",
            "Epoch 15/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.6194 - acc: 0.8346 - val_loss: 0.6106 - val_acc: 0.8676\n",
            "Epoch 16/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.6130 - acc: 0.8258 - val_loss: 0.6046 - val_acc: 0.8767\n",
            "Epoch 17/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.6064 - acc: 0.8454 - val_loss: 0.5979 - val_acc: 0.8721\n",
            "Epoch 18/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.5997 - acc: 0.8542 - val_loss: 0.5902 - val_acc: 0.8767\n",
            "Epoch 19/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.5927 - acc: 0.8493 - val_loss: 0.5829 - val_acc: 0.8721\n",
            "Epoch 20/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.5852 - acc: 0.8503 - val_loss: 0.5759 - val_acc: 0.8676\n",
            "Epoch 21/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.5777 - acc: 0.8562 - val_loss: 0.5671 - val_acc: 0.8676\n",
            "Epoch 22/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.5700 - acc: 0.8571 - val_loss: 0.5591 - val_acc: 0.8676\n",
            "Epoch 23/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.5619 - acc: 0.8591 - val_loss: 0.5507 - val_acc: 0.8630\n",
            "Epoch 24/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.5537 - acc: 0.8630 - val_loss: 0.5423 - val_acc: 0.8630\n",
            "Epoch 25/120\n",
            "1022/1022 [==============================] - 0s 56us/step - loss: 0.5452 - acc: 0.8611 - val_loss: 0.5325 - val_acc: 0.8676\n",
            "Epoch 26/120\n",
            "1022/1022 [==============================] - 0s 56us/step - loss: 0.5368 - acc: 0.8620 - val_loss: 0.5249 - val_acc: 0.8721\n",
            "Epoch 27/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.5281 - acc: 0.8611 - val_loss: 0.5155 - val_acc: 0.8721\n",
            "Epoch 28/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.5195 - acc: 0.8630 - val_loss: 0.5055 - val_acc: 0.8630\n",
            "Epoch 29/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.5108 - acc: 0.8640 - val_loss: 0.4961 - val_acc: 0.8630\n",
            "Epoch 30/120\n",
            "1022/1022 [==============================] - 0s 52us/step - loss: 0.5018 - acc: 0.8620 - val_loss: 0.4882 - val_acc: 0.8721\n",
            "Epoch 31/120\n",
            "1022/1022 [==============================] - 0s 54us/step - loss: 0.4932 - acc: 0.8620 - val_loss: 0.4777 - val_acc: 0.8767\n",
            "Epoch 32/120\n",
            "1022/1022 [==============================] - 0s 57us/step - loss: 0.4844 - acc: 0.8620 - val_loss: 0.4690 - val_acc: 0.8767\n",
            "Epoch 33/120\n",
            "1022/1022 [==============================] - 0s 56us/step - loss: 0.4761 - acc: 0.8620 - val_loss: 0.4606 - val_acc: 0.8676\n",
            "Epoch 34/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.4674 - acc: 0.8640 - val_loss: 0.4509 - val_acc: 0.8767\n",
            "Epoch 35/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.4594 - acc: 0.8591 - val_loss: 0.4419 - val_acc: 0.8767\n",
            "Epoch 36/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.4512 - acc: 0.8601 - val_loss: 0.4326 - val_acc: 0.8721\n",
            "Epoch 37/120\n",
            "1022/1022 [==============================] - 0s 43us/step - loss: 0.4436 - acc: 0.8611 - val_loss: 0.4284 - val_acc: 0.8493\n",
            "Epoch 38/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.4358 - acc: 0.8630 - val_loss: 0.4222 - val_acc: 0.8447\n",
            "Epoch 39/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.4289 - acc: 0.8640 - val_loss: 0.4124 - val_acc: 0.8584\n",
            "Epoch 40/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.4220 - acc: 0.8669 - val_loss: 0.4039 - val_acc: 0.8630\n",
            "Epoch 41/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.4153 - acc: 0.8640 - val_loss: 0.3959 - val_acc: 0.8721\n",
            "Epoch 42/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.4090 - acc: 0.8630 - val_loss: 0.3910 - val_acc: 0.8584\n",
            "Epoch 43/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.4030 - acc: 0.8650 - val_loss: 0.3826 - val_acc: 0.8767\n",
            "Epoch 44/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.3973 - acc: 0.8650 - val_loss: 0.3773 - val_acc: 0.8676\n",
            "Epoch 45/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.3918 - acc: 0.8630 - val_loss: 0.3743 - val_acc: 0.8493\n",
            "Epoch 46/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3867 - acc: 0.8640 - val_loss: 0.3674 - val_acc: 0.8539\n",
            "Epoch 47/120\n",
            "1022/1022 [==============================] - 0s 52us/step - loss: 0.3818 - acc: 0.8679 - val_loss: 0.3655 - val_acc: 0.8447\n",
            "Epoch 48/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3772 - acc: 0.8659 - val_loss: 0.3571 - val_acc: 0.8584\n",
            "Epoch 49/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.3730 - acc: 0.8669 - val_loss: 0.3526 - val_acc: 0.8584\n",
            "Epoch 50/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.3692 - acc: 0.8689 - val_loss: 0.3480 - val_acc: 0.8630\n",
            "Epoch 51/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.3648 - acc: 0.8679 - val_loss: 0.3426 - val_acc: 0.8676\n",
            "Epoch 52/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3613 - acc: 0.8699 - val_loss: 0.3376 - val_acc: 0.8721\n",
            "Epoch 53/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3577 - acc: 0.8699 - val_loss: 0.3356 - val_acc: 0.8630\n",
            "Epoch 54/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.3543 - acc: 0.8699 - val_loss: 0.3356 - val_acc: 0.8493\n",
            "Epoch 55/120\n",
            "1022/1022 [==============================] - 0s 43us/step - loss: 0.3510 - acc: 0.8718 - val_loss: 0.3377 - val_acc: 0.8447\n",
            "Epoch 56/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.3491 - acc: 0.8718 - val_loss: 0.3279 - val_acc: 0.8539\n",
            "Epoch 57/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.3457 - acc: 0.8718 - val_loss: 0.3267 - val_acc: 0.8584\n",
            "Epoch 58/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3430 - acc: 0.8718 - val_loss: 0.3205 - val_acc: 0.8539\n",
            "Epoch 59/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.3406 - acc: 0.8699 - val_loss: 0.3183 - val_acc: 0.8493\n",
            "Epoch 60/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.3382 - acc: 0.8738 - val_loss: 0.3142 - val_acc: 0.8630\n",
            "Epoch 61/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.3363 - acc: 0.8777 - val_loss: 0.3130 - val_acc: 0.8584\n",
            "Epoch 62/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.3343 - acc: 0.8728 - val_loss: 0.3135 - val_acc: 0.8630\n",
            "Epoch 63/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.3321 - acc: 0.8767 - val_loss: 0.3145 - val_acc: 0.8630\n",
            "Epoch 64/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.3312 - acc: 0.8748 - val_loss: 0.3136 - val_acc: 0.8676\n",
            "Epoch 65/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3287 - acc: 0.8748 - val_loss: 0.3051 - val_acc: 0.8630\n",
            "Epoch 66/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.3268 - acc: 0.8777 - val_loss: 0.3036 - val_acc: 0.8630\n",
            "Epoch 67/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3246 - acc: 0.8767 - val_loss: 0.2975 - val_acc: 0.8676\n",
            "Epoch 68/120\n",
            "1022/1022 [==============================] - 0s 43us/step - loss: 0.3237 - acc: 0.8777 - val_loss: 0.3015 - val_acc: 0.8630\n",
            "Epoch 69/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.3220 - acc: 0.8806 - val_loss: 0.2976 - val_acc: 0.8630\n",
            "Epoch 70/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.3205 - acc: 0.8806 - val_loss: 0.2943 - val_acc: 0.8721\n",
            "Epoch 71/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.3194 - acc: 0.8796 - val_loss: 0.2959 - val_acc: 0.8630\n",
            "Epoch 72/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3182 - acc: 0.8816 - val_loss: 0.2908 - val_acc: 0.8721\n",
            "Epoch 73/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.3169 - acc: 0.8777 - val_loss: 0.2918 - val_acc: 0.8630\n",
            "Epoch 74/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.3156 - acc: 0.8767 - val_loss: 0.2936 - val_acc: 0.8767\n",
            "Epoch 75/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3146 - acc: 0.8796 - val_loss: 0.2871 - val_acc: 0.8721\n",
            "Epoch 76/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.3134 - acc: 0.8777 - val_loss: 0.2869 - val_acc: 0.8721\n",
            "Epoch 77/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3121 - acc: 0.8767 - val_loss: 0.2890 - val_acc: 0.8767\n",
            "Epoch 78/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.3114 - acc: 0.8836 - val_loss: 0.2845 - val_acc: 0.8813\n",
            "Epoch 79/120\n",
            "1022/1022 [==============================] - 0s 57us/step - loss: 0.3107 - acc: 0.8787 - val_loss: 0.2892 - val_acc: 0.8767\n",
            "Epoch 80/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3090 - acc: 0.8816 - val_loss: 0.2830 - val_acc: 0.8767\n",
            "Epoch 81/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.3085 - acc: 0.8777 - val_loss: 0.2824 - val_acc: 0.8767\n",
            "Epoch 82/120\n",
            "1022/1022 [==============================] - 0s 52us/step - loss: 0.3073 - acc: 0.8787 - val_loss: 0.2828 - val_acc: 0.8767\n",
            "Epoch 83/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.3061 - acc: 0.8777 - val_loss: 0.2839 - val_acc: 0.8767\n",
            "Epoch 84/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.3058 - acc: 0.8836 - val_loss: 0.2808 - val_acc: 0.8767\n",
            "Epoch 85/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.3046 - acc: 0.8796 - val_loss: 0.2775 - val_acc: 0.8858\n",
            "Epoch 86/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.3039 - acc: 0.8777 - val_loss: 0.2782 - val_acc: 0.8767\n",
            "Epoch 87/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.3032 - acc: 0.8806 - val_loss: 0.2757 - val_acc: 0.8858\n",
            "Epoch 88/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.3027 - acc: 0.8806 - val_loss: 0.2726 - val_acc: 0.8904\n",
            "Epoch 89/120\n",
            "1022/1022 [==============================] - 0s 53us/step - loss: 0.3018 - acc: 0.8806 - val_loss: 0.2741 - val_acc: 0.8904\n",
            "Epoch 90/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.3011 - acc: 0.8796 - val_loss: 0.2694 - val_acc: 0.8950\n",
            "Epoch 91/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.3005 - acc: 0.8796 - val_loss: 0.2686 - val_acc: 0.8904\n",
            "Epoch 92/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.2999 - acc: 0.8845 - val_loss: 0.2691 - val_acc: 0.8950\n",
            "Epoch 93/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.2991 - acc: 0.8816 - val_loss: 0.2674 - val_acc: 0.8904\n",
            "Epoch 94/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.2988 - acc: 0.8855 - val_loss: 0.2696 - val_acc: 0.8904\n",
            "Epoch 95/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.2974 - acc: 0.8845 - val_loss: 0.2666 - val_acc: 0.8904\n",
            "Epoch 96/120\n",
            "1022/1022 [==============================] - 0s 50us/step - loss: 0.2971 - acc: 0.8826 - val_loss: 0.2682 - val_acc: 0.8904\n",
            "Epoch 97/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.2964 - acc: 0.8826 - val_loss: 0.2678 - val_acc: 0.8904\n",
            "Epoch 98/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.2956 - acc: 0.8836 - val_loss: 0.2650 - val_acc: 0.8904\n",
            "Epoch 99/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.2955 - acc: 0.8855 - val_loss: 0.2731 - val_acc: 0.8813\n",
            "Epoch 100/120\n",
            "1022/1022 [==============================] - 0s 44us/step - loss: 0.2943 - acc: 0.8865 - val_loss: 0.2732 - val_acc: 0.8813\n",
            "Epoch 101/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.2938 - acc: 0.8865 - val_loss: 0.2644 - val_acc: 0.8858\n",
            "Epoch 102/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.2934 - acc: 0.8826 - val_loss: 0.2658 - val_acc: 0.8904\n",
            "Epoch 103/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.2928 - acc: 0.8826 - val_loss: 0.2634 - val_acc: 0.8858\n",
            "Epoch 104/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.2921 - acc: 0.8885 - val_loss: 0.2662 - val_acc: 0.8813\n",
            "Epoch 105/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.2912 - acc: 0.8845 - val_loss: 0.2691 - val_acc: 0.8813\n",
            "Epoch 106/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.2913 - acc: 0.8855 - val_loss: 0.2668 - val_acc: 0.8813\n",
            "Epoch 107/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.2903 - acc: 0.8865 - val_loss: 0.2585 - val_acc: 0.8904\n",
            "Epoch 108/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.2908 - acc: 0.8875 - val_loss: 0.2605 - val_acc: 0.8858\n",
            "Epoch 109/120\n",
            "1022/1022 [==============================] - 0s 51us/step - loss: 0.2897 - acc: 0.8865 - val_loss: 0.2619 - val_acc: 0.8858\n",
            "Epoch 110/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.2890 - acc: 0.8875 - val_loss: 0.2612 - val_acc: 0.8858\n",
            "Epoch 111/120\n",
            "1022/1022 [==============================] - 0s 49us/step - loss: 0.2885 - acc: 0.8885 - val_loss: 0.2604 - val_acc: 0.8858\n",
            "Epoch 112/120\n",
            "1022/1022 [==============================] - 0s 47us/step - loss: 0.2877 - acc: 0.8875 - val_loss: 0.2620 - val_acc: 0.8767\n",
            "Epoch 113/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.2867 - acc: 0.8855 - val_loss: 0.2581 - val_acc: 0.8858\n",
            "Epoch 114/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.2875 - acc: 0.8855 - val_loss: 0.2565 - val_acc: 0.8858\n",
            "Epoch 115/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.2877 - acc: 0.8836 - val_loss: 0.2597 - val_acc: 0.8813\n",
            "Epoch 116/120\n",
            "1022/1022 [==============================] - 0s 46us/step - loss: 0.2862 - acc: 0.8865 - val_loss: 0.2560 - val_acc: 0.8858\n",
            "Epoch 117/120\n",
            "1022/1022 [==============================] - 0s 48us/step - loss: 0.2858 - acc: 0.8875 - val_loss: 0.2640 - val_acc: 0.8767\n",
            "Epoch 118/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.2862 - acc: 0.8885 - val_loss: 0.2604 - val_acc: 0.8767\n",
            "Epoch 119/120\n",
            "1022/1022 [==============================] - 0s 52us/step - loss: 0.2856 - acc: 0.8875 - val_loss: 0.2542 - val_acc: 0.8904\n",
            "Epoch 120/120\n",
            "1022/1022 [==============================] - 0s 45us/step - loss: 0.2848 - acc: 0.8875 - val_loss: 0.2564 - val_acc: 0.8858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0d238cf898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzI_EtJZgCKE",
        "colab_type": "code",
        "outputId": "01cade05-e468-46f6-d603-fe3fd01de1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "219/219 [==============================] - 0s 54us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.21776613069179396, 0.922374427318573]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTGArbQBynew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}